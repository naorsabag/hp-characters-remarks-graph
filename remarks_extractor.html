<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8" />
  <title>HP Character Remarks Extractor</title>
  <style>
    body {
      font-family: sans-serif;
      margin: 20px;
    }
    #progress {
      font-weight: bold;
      margin-bottom: 10px;
    }
    pre {
      background: #f4f4f4;
      padding: 10px;
      border-radius: 6px;
      max-height: 400px;
      overflow: auto;
    }
  </style>
</head>
<body>
  <h1>Harry Potter Character Remarks Extractor</h1>

  <div style="margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 6px; background-color: #f9f9f9;">
    <label for="apiKeyInput" style="display: block; margin-bottom: 8px; font-weight: bold;">OpenAI API Key:</label>
    <input
      type="password"
      id="apiKeyInput"
      placeholder="Enter your OpenAI API key (sk-...)"
      style="width: 100%; max-width: 500px; padding: 8px; border: 1px solid #ccc; border-radius: 4px; margin-bottom: 8px;"
    />
    <div style="font-size: 12px; color: #666;">
      Your API key is stored only in your browser and is never sent anywhere except to OpenAI's servers.
    </div>
  </div>

  <div style="margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 6px; background-color: #fff8dc;">
    <label for="characterNamesFile" style="display: block; margin-bottom: 8px; font-weight: bold;">Character Names File (Optional):</label>
    <input
      type="file"
      id="characterNamesFile"
      accept=".json"
      style="width: 100%; max-width: 500px; padding: 8px; border: 1px solid #ccc; border-radius: 4px; margin-bottom: 8px;"
    />
    <div style="font-size: 12px; color: #666;">
      Upload character_names.json to use the full character list (274 characters). If not provided, a minimal fallback list will be used.
    </div>
    <div id="characterNamesStatus" style="font-size: 12px; margin-top: 8px; font-weight: bold;"></div>
  </div>

  <div style="margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 6px; background-color: #f9f9f9;">
    <label for="bookFiles" style="display: block; margin-bottom: 8px; font-weight: bold;">Select Harry Potter Book Files:</label>
    <input
      type="file"
      id="bookFiles"
      multiple
      accept=".txt"
      style="width: 100%; max-width: 500px; padding: 8px; border: 1px solid #ccc; border-radius: 4px; margin-bottom: 8px;"
    />
    <div style="font-size: 12px; color: #666;">
      Select all 7 book text files (Book1.txt, Book2.txt, etc.). Files are processed locally in your browser.
    </div>
  </div>

  <div style="margin-bottom: 20px; padding: 15px; border: 1px solid #ddd; border-radius: 6px; background-color: #f0f8ff;">
    <label for="jsonFile" style="display: block; margin-bottom: 8px; font-weight: bold;">Or Upload Extracted Remarks JSON for Sentiment Analysis:</label>
    <input
      type="file"
      id="jsonFile"
      accept=".json"
      style="width: 100%; max-width: 500px; padding: 8px; border: 1px solid #ccc; border-radius: 4px; margin-bottom: 8px;"
    />
    <div style="font-size: 12px; color: #666;">
      Alternative workflow: If you already have extracted remarks from the books, upload the JSON file here for sentiment analysis.<br>
      <strong>Supported formats:</strong> 
      <ul style="margin: 5px 0; padding-left: 20px;">
        <li>New format: JSON with "chunks" array (flat list of chunks)</li>
        <li>Old format: JSON with "successful_chunks" nested structure (backward compatibility)</li>
      </ul>
      The JSON should contain extracted remarks with character_name, target, description, and chunk_text for each remark.
    </div>
  </div>

  <div id="progress">Waiting to start...</div>
  <button id="startBtn">Extract Remarks from Books</button>
  <button id="analyzeSentimentBtn">Analyze Sentiment from Data</button>

  <!-- Download button - initially hidden -->
  <div id="downloadSection" style="margin: 20px 0; padding: 15px; border: 1px solid #4CAF50; border-radius: 6px; background-color: #f8fff8; display: none;">
    <h3 style="margin-bottom: 10px; color: #2e7d32;">Download Results</h3>
    <div style="display: flex; gap: 10px; flex-wrap: wrap;">
      <button id="downloadBtn" style="padding: 8px 16px; background-color: #4CAF50; color: white; border: none; border-radius: 4px; cursor: pointer; font-size: 14px;">Download Results</button>
    </div>
    <div id="downloadInfo" style="font-size: 12px; color: #666; margin-top: 8px;"></div>
  </div>

  <pre id="output"></pre>

  <script type="module">
    // System prompts
    const REMARKS_EXTRACTION_SYSTEM_PROMPT = `
You are an expert text analyzer. Analyze the provided Harry Potter text and extract ALL remarks and comments made by characters:
- Read the entire text carefully, ensuring no comment or remarks is skipped.
- Include remarks or comments made by any character mentioned in the text, not just the main ones.
- Include remarks or comments made on any kind of group of people or animals or objects, for instance family, musicians, gingers, irish, cats, table....
- Include remarks or comments made behind the back where the subject wasn't present.
- Include remarks or comments happened in the character head, means that it wasn't said out loud but the character only thought it.
- Do not include remarks stated by the narrator or descriptive text.
- Only extract remarks from the provided text - do not use external knowledge.
- When context from a previous section is provided, use it to better understand ongoing conversations and situations, but extract remarks ONLY from the current text section.
- Use the exact character names as they appear in the text for both character_name (who said it) and target (who it was about).
- If the character made a remark about a group, include all the members of the group in the target.
- If the character made a remark someone's belonging or relatives include both the person and the belonging or relatives in the target.
- For each remark, provide a brief description of what was said (1-2 sentences maximum).
- Output strictly as JSON.
- Keep the descriptions short and concise.
    `;

    const SENTIMENT_ANALYSIS_SYSTEM_PROMPT = `
You are an expert Harry Potter sentiment analyzer. You will analyze a batch of remarks/comments from the same paragraph and determine the sentiment of the speaker towards their target.

For each item in the batch:
- Analyze the remark to understand what the speaker said or thought about the target
- Use the overall paragraph context provided at the beginning to better understand the situation, tone, and relationship between characters.
- Determine if the sentiment of the speaker towards the target in the specific provided remark is positive, negative, or neutral.`;

    const CHARACTER_CONVERSION_SYSTEM_PROMPT = `
You are a Harry Potter character name converter. Your job is to take character names and convert them to their canonical match from the predefined list.

For each character name, find the best match from the valid characters list. Special cases:

1. **Families/Groups**: If the name refers to a family or group (e.g., "Ron's family", "the Weasleys", "Hermione's parents"), return ALL individual members from that group that are in the valid list.

2. **Self-references**: If the name refers to the speaker themselves (e.g., "myself", "me", "I", "him", "her"), use the context provided to identify who the speaker is and map it to that character.

3. **Belongings/Relatives**: If the name refers to someone's belongings or relatives (e.g., "Harry's owl", "Hermione's parents", "Ron's rat"), include both the owner AND the belonging/relative if in the valid list.

4. **Regular names**: For regular character names, find the closest match (e.g., "Dumbledore" -> "Albus Dumbledore").

5. **Unknown/Generic**: If no good match exists, use broader categories like "Students", "Wizards", "Muggles", etc., or "other" as last resort.

Return a mapping where each original name maps to an array of valid character names (even if it's just one character, use an array).
    `;

    // User message templates
    function createRemarksExtractionUserMessage(chunkText, previousChunkText = null) {
      if (previousChunkText) {
        return `Here is some context from the previous section for continuity:
"""
${previousChunkText}
"""

Now extract ALL remarks and comments made by any character from this current text:
"""
${chunkText}
"""

Note: The previous section is provided only for context. Extract remarks ONLY from the current text, not from the context section.`;
      } else {
        return `Extract ALL remarks and comments made by any character from this text:
"""
${chunkText}
"""`;
      }
    }

    function createSentimentAnalysisUserMessage(batchData, chunkInfo = null) {
      const chunkContext = chunkInfo ? `The remarks where extracted from this text:
"""
${chunkInfo.chunk_text}
"""

` : '';
      
      const bookInfo = chunkInfo ? `Book ${chunkInfo.book}` : '';
      
      const remarksData = batchData.map((item, index) => `
Item index: ${index},
Speaker: ${item.speaker},
Target: ${item.target},
The remark made by ${item.speaker} about ${item.target}: "${item.description}"
`).join('\n');

      return `${chunkContext}Analyze the sentiment for each of following remarks from ${bookInfo}:

${remarksData}`;
    }

    function createCharacterConversionUserMessage(namesToConvert, contextData) {
      return `Convert these character names to match the valid character list:

Names to convert: ${JSON.stringify(namesToConvert, null, 2)}

Context (for resolving self-references and belongings):
${JSON.stringify(contextData, null, 2)}

Use the context to resolve self-references like "myself", "me", "I", "him", "her" to the actual speaker.
For belongings/relatives (e.g., "Harry's owl", "his parents"), include both the owner and the belonging/relative in the mapped array.`;
    }

    function getApiKey() {
      return document.getElementById('apiKeyInput').value.trim();
    }

    // Load character names from JSON file
    let validCharacters = [];

    // Initialize with fallback character names
    function initializeFallbackCharacters() {
      validCharacters = [
        "Harry Potter", "Hermione Granger", "Ron Weasley", "Albus Dumbledore", "Severus Snape",
        "Draco Malfoy", "Voldemort", "Rubeus Hagrid", "Minerva McGonagall", "Sirius Black",
        "Students", "Wizards", "Death Eaters", "Order of the Phoenix", "other"
      ];
      updateCharacterNamesStatus(`Using fallback character list (${validCharacters.length} characters)`, 'orange');
      console.warn('⚠️ Using fallback character list with', validCharacters.length, 'characters');
    }

    // Load character names from uploaded file
    async function loadCharacterNamesFromFile(file) {
      try {
        const fileText = await file.text();
        const data = JSON.parse(fileText);

        if (!data.character_names || !Array.isArray(data.character_names)) {
          throw new Error('Invalid JSON format: missing character_names array');
        }

        validCharacters = data.character_names;
        updateCharacterNamesStatus(`✅ Loaded ${validCharacters.length} characters from ${file.name}`, 'green');
        console.log(`✅ Loaded ${validCharacters.length} character names from ${file.name}`);
        return true;
      } catch (error) {
        console.error('❌ Failed to load character names from file:', error);
        updateCharacterNamesStatus(`❌ Failed to load ${file.name}: ${error.message}`, 'red');
        return false;
      }
    }

    // Update character names status display
    function updateCharacterNamesStatus(message, color) {
      const statusEl = document.getElementById('characterNamesStatus');
      statusEl.textContent = message;
      statusEl.style.color = color;
    }

    // Function to read a book file from File object
    async function readBookFile(file) {
      try {
        return await file.text();
      } catch (error) {
        console.error(`Error reading ${file.name}:`, error);
        throw error;
      }
    }


    // Function to parse JSON file and return chunk-based structure for sentiment analysis
    async function parseJSONFile(file) {
      try {
        const jsonText = await file.text();
        const jsonData = JSON.parse(jsonText);

        const chunks = [];

        // Check for new flat format first
        if (jsonData.chunks && Array.isArray(jsonData.chunks)) {
          // New flat format
          jsonData.chunks.forEach((chunkData, index) => {
            const chunkText = chunkData.chunk_text || '';
            const remarks = chunkData.remarks || [];
            const book = chunkData.book || 'unknown';
            const chunkIndex = chunkData.chunk_index !== undefined ? chunkData.chunk_index : index;

            if (remarks.length > 0) {
              // Split multi-target remarks into individual entries
              const splitRemarks = [];
              
              remarks.forEach(remark => {
                if (Array.isArray(remark.target) && remark.target.length > 1) {
                  // Split multi-target remark into separate entries
                  remark.target.forEach(target => {
                    splitRemarks.push({
                      book: book.toString(),
                      speaker: remark.character_name || '',
                      target: target,
                      sentiment: '', // Empty, to be filled by sentiment analysis
                      description: remark.description || ''
                    });
                  });
                } else {
                  // Single target
                  splitRemarks.push({
                    book: book.toString(),
                    speaker: remark.character_name || '',
                    target: Array.isArray(remark.target) ? remark.target[0] : remark.target || '',
                    sentiment: '', // Empty, to be filled by sentiment analysis
                    description: remark.description || ''
                  });
                }
              });

              // Create a chunk batch with all split remarks from this chunk
              const chunkBatch = {
                book: book.toString(),
                chunkKey: `Chunk ${chunkIndex}`,
                chunk_text: chunkText,
                remarks: splitRemarks
              };
              chunks.push(chunkBatch);
            }
          });
        }
        // Backward compatibility: check for old nested format
        else if (jsonData.successful_chunks) {
          // Old nested format
          Object.entries(jsonData.successful_chunks).forEach(([bookKey, bookData]) => {
            // Extract book number from key like "Book 1"
            const bookMatch = bookKey.match(/Book (\d+)/);
            const bookNumber = bookMatch ? bookMatch[1] : 'unknown';

            Object.entries(bookData).forEach(([chunkKey, chunkData]) => {
              const chunkText = chunkData.chunk_text || '';
              const remarks = chunkData.remarks || [];

              if (remarks.length > 0) {
                // Split multi-target remarks into individual entries
                const splitRemarks = [];
                
                remarks.forEach(remark => {
                  if (Array.isArray(remark.target) && remark.target.length > 1) {
                    // Split multi-target remark into separate entries
                    remark.target.forEach(target => {
                      splitRemarks.push({
                        book: bookNumber,
                        speaker: remark.character_name || '',
                        target: target,
                        sentiment: '', // Empty, to be filled by sentiment analysis
                        description: remark.description || ''
                      });
                    });
                  } else {
                    // Single target
                    splitRemarks.push({
                      book: bookNumber,
                      speaker: remark.character_name || '',
                      target: Array.isArray(remark.target) ? remark.target[0] : remark.target || '',
                      sentiment: '', // Empty, to be filled by sentiment analysis
                      description: remark.description || ''
                    });
                  }
                });

                // Create a chunk batch with all split remarks from this chunk
                const chunkBatch = {
                  book: bookNumber,
                  chunkKey: chunkKey,
                  chunk_text: chunkText,
                  remarks: splitRemarks
                };
                chunks.push(chunkBatch);
              }
            });
          });
        }
        else {
          throw new Error('JSON file must contain either "chunks" array (new format) or "successful_chunks" property (old format) with the remarks extraction format');
        }

        return chunks;
      } catch (error) {
        console.error(`Error parsing JSON file ${file.name}:`, error);
        throw error;
      }
    }

    // Function to split text into chunks of approximately 1000 words, respecting paragraph boundaries
    function chunkText(text, maxWords = 1000) {
      const chunks = [];
      // Split by double newlines (paragraphs)
      const paragraphs = text.split('\n\n');

      let currentChunk = '';
      let currentWordCount = 0;

      for (const paragraph of paragraphs) {
        const paragraphWords = paragraph.trim().split(/\s+/).length;

        // If adding this paragraph would exceed the limit and we already have content
        if (currentWordCount + paragraphWords > maxWords && currentChunk.length > 0) {
          // Save current chunk and start a new one
          chunks.push(currentChunk.trim());
          currentChunk = paragraph;
          currentWordCount = paragraphWords;
        } else {
          // Add paragraph to current chunk
          if (currentChunk.length > 0) {
            currentChunk += '\n\n' + paragraph;
          } else {
            currentChunk = paragraph;
          }
          currentWordCount += paragraphWords;
        }
      }

      // Add the last chunk if it has content
      if (currentChunk.trim().length > 0) {
        chunks.push(currentChunk.trim());
      }

      return chunks;
    }

    async function makeApiCall(book, chunkIndex, chunkText, previousChunkText = null) {
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Request timeout after 30 seconds')), 30000);
      });

      const apiCallPromise = async () => {
        // Create user message for debugging
        const userMessage = createRemarksExtractionUserMessage(chunkText, previousChunkText);
        
        // Console log for debugging (uncomment when needed)
        // console.log(`[DEBUG] Remarks extraction user message for Book ${book}, Chunk ${chunkIndex}:`, userMessage);
        
        const response = await fetch("https://api.openai.com/v1/chat/completions", {
          method: "POST",
          headers: {
            "Authorization": `Bearer ${getApiKey()}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: "gpt-4o",
            messages: [
              {
                role: "system",
                content: REMARKS_EXTRACTION_SYSTEM_PROMPT
              },
              {
                role: "user",
                content: userMessage
              }
            ],
            temperature: 0,
            response_format: {
              type: "json_schema",
              json_schema: {
                  name: "remarks_schema",
                  strict: true,
                  schema: {
                      type: "object",
                      properties: {
                          book: {
                              type: "integer",
                          },
                          remarks: {
                              type: "array",
                              description: "A list of ALL remarks or comments stated by any character.",
                              items: {
                                  type: "object",
                                  properties: {
                                      character_name: {
                                          type: "string",
                                          description: "The character who said the remark.",
                                      },
                                      target: {
                                          type: "array",
                                          items: {
                                              type: "string"
                                          },
                                          description: "The character(s) or group(s) or animal(s) or object(s) who were the subject of the remark. could be also someone's family, friends, animals, objects, etc. in that case need to include the owner or the relative in the target.",
                                      },
                                      description: {
                                          type: "string",
                                          description: "A short description of the remark or comment made by the character.",
                                      },
                                  },
                                  required: ["character_name", "target", "description"],
                                  additionalProperties: false,
                              },
                          },
                      },
                      required: ["book", "remarks"],
                      additionalProperties: false,
                  },

              }
            }
          })
        });

        const completion = await response.json();

        try {
          const parsed = JSON.parse(completion.choices[0].message.content);
          // Add chunk text to the parsed data
          parsed.chunk_text = chunkText;
          return { success: true, data: parsed, book, chunkIndex };
        } catch (err) {
          console.error(`Failed to parse JSON for book ${book}, chunk ${chunkIndex}:`, err);
          return { success: false, error: `Parse error: ${err.message}`, book, chunkIndex };
        }
      };

      try {
        return await Promise.race([apiCallPromise(), timeoutPromise]);
      } catch (err) {
        console.error(`API call failed for book ${book}, chunk ${chunkIndex}:`, err);
        return { success: false, error: err.message, book, chunkIndex };
      }
    }

    async function makeSentimentAnalysisCall(batchData, batchIndex, chunkInfo = null) {
      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Request timeout after 30 seconds')), 30000);
      });

      const apiCallPromise = async () => {
        // Create user message for debugging
        const userMessage = createSentimentAnalysisUserMessage(batchData, chunkInfo);
        
        // Console log for debugging (uncomment when needed)
        // console.log(`[DEBUG] Sentiment analysis user message for batch ${batchIndex}:`, userMessage);
        
        const response = await fetch("https://api.openai.com/v1/chat/completions", {
          method: "POST",
          headers: {
            "Authorization": `Bearer ${getApiKey()}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: "gpt-4o",
            messages: [
              {
                role: "system",
                content: SENTIMENT_ANALYSIS_SYSTEM_PROMPT
              },
              {
                role: "user",
                content: userMessage
              }
            ],
            temperature: 0,
            response_format: {
              type: "json_schema",
              json_schema: {
                name: "sentiment_analysis_schema",
                strict: true,
                schema: {
                  type: "object",
                  properties: {
                    batch_index: {
                      type: "integer"
                    },
                    analyzed_items: {
                      type: "array",
                      description: "Analysis results for each item in the batch",
                      items: {
                        type: "object",
                        properties: {
                          item_index: {
                            type: "integer",
                            description: "The index of this item within the batch (0-based) according to 'Item index' value"
                          },
                          sentiment: {
                            type: "string",
                            enum: ["positive", "negative", "neutral"],
                            description: "Does the speaker said/though something positive, neutral or negative about the target, in the specific remark"
                          },
                          reasoning: {
                            type: "string",
                            description: "Brief explanation for the sentiment classification"
                          }
                        },
                        required: ["item_index", "sentiment",  "reasoning"],
                        additionalProperties: false
                      }
                    }
                  },
                  required: ["batch_index", "analyzed_items"],
                  additionalProperties: false
                }
              }
            }
          })
        });

        const completion = await response.json();

        try {
          const parsed = JSON.parse(completion.choices[0].message.content);
          return { success: true, data: parsed, batchIndex };
        } catch (err) {
          console.error(`Failed to parse JSON for sentiment batch ${batchIndex}:`, err);
          return { success: false, error: `Parse error: ${err.message}`, batchIndex };
        }
      };

      try {
        return await Promise.race([apiCallPromise(), timeoutPromise]);
      } catch (err) {
        console.error(`Sentiment analysis API call failed for batch ${batchIndex}:`, err);
        return { success: false, error: err.message, batchIndex };
      }
    }

    async function convertCharacterNames(remarkData) {
      // Extract all unique character names from the data
      const uniqueNames = new Set();
      const contextData = [];

      remarkData.remarks.forEach(remark => {
        uniqueNames.add(remark.character_name);
        remark.target.forEach(target => uniqueNames.add(target));

        // Store context for self-reference resolution
        contextData.push({
          speaker: remark.character_name,
          targets: remark.target,
          description: remark.description
        });
      });

      const namesToConvert = Array.from(uniqueNames).filter(name =>
        !validCharacters.includes(name)
      );

      // If no names need conversion, return empty mapping
      if (namesToConvert.length === 0) {
        return { success: true, mapping: {} };
      }

      const timeoutPromise = new Promise((_, reject) => {
        setTimeout(() => reject(new Error('Character conversion timeout after 30 seconds')), 30000);
      });

      const conversionPromise = async () => {
        // Create user message for debugging
        const userMessage = createCharacterConversionUserMessage(namesToConvert, contextData);
        
        // Console log for debugging (uncomment when needed)
        // console.log(`[DEBUG] Character conversion user message:`, userMessage);
        
        const response = await fetch("https://api.openai.com/v1/chat/completions", {
          method: "POST",
          headers: {
            "Authorization": `Bearer ${getApiKey()}`,
            "Content-Type": "application/json"
          },
          body: JSON.stringify({
            model: "gpt-4o",
            messages: [
              {
                role: "system",
                content: CHARACTER_CONVERSION_SYSTEM_PROMPT
              },
              {
                role: "user",
                content: userMessage
              }
            ],
            temperature: 0,
            response_format: {
              type: "json_schema",
              json_schema: {
                name: "character_name_mapping",
                strict: false,
                schema: {
                  type: "object",
                  properties: {
                    mapping: {
                      type: "object",
                      description: "A mapping of original character names to arrays of valid character names from the enum list",
                      additionalProperties: {
                        type: "array",
                        items: {
                          type: "string",
                          enum: validCharacters,
                          description: "The valid character name list to choose from"
                        }
                      }
                    }
                  },
                  required: ["mapping"],
                  additionalProperties: false
                }
              }
            }
          })
        });

        const completion = await response.json();

        try {
          const parsed = JSON.parse(completion.choices[0].message.content);
          return { success: true, mapping: parsed.mapping };
        } catch (err) {
          console.error(`Failed to parse conversion JSON:`, err);
          return { success: false, error: `Parse error: ${err.message}` };
        }
      };

      try {
        return await Promise.race([conversionPromise(), timeoutPromise]);
      } catch (err) {
        console.error(`Character conversion failed:`, err);
        return { success: false, error: err.message };
      }
    }

    function applyCharacterMapping(remarkData, mapping) {
      // Create a deep copy of the original data
      const updatedData = JSON.parse(JSON.stringify(remarkData));

      // Apply mapping to character_name and target fields
      updatedData.remarks.forEach(remark => {
        const originalSpeaker = remark.character_name;

        // For character_name (who said it), use the first character from the mapping array
        // since a remark can only be said by one character at a time
        if (mapping[remark.character_name] && Array.isArray(mapping[remark.character_name])) {
          remark.character_name = mapping[remark.character_name][0];
        }

        // For target (who it was about), expand to all mapped characters
        const expandedTargets = [];
        const originalTargets = [...remark.target]; // Store original targets

        remark.target.forEach(target => {
          if (mapping[target] && Array.isArray(mapping[target])) {
            // Add all characters from the mapping array
            expandedTargets.push(...mapping[target]);
          } else {
            // Keep original target if no mapping found
            expandedTargets.push(target);
          }
        });

        // Special handling: If any original target implied the speaker's belongings/relatives
        // (like "my family", "his owl", etc.), the LLM mapping should have included the speaker,
        // but let's ensure the final mapped speaker is also in the targets if it makes sense
        const finalSpeaker = remark.character_name;
        const shouldIncludeSpeaker = originalTargets.some(target =>
          target.toLowerCase().includes('my ') ||
          target.toLowerCase().includes('myself') ||
          target.toLowerCase().includes('me ') ||
          (mapping[target] && mapping[target].includes(finalSpeaker))
        );

        if (shouldIncludeSpeaker && !expandedTargets.includes(finalSpeaker)) {
          expandedTargets.push(finalSpeaker);
        }

        // Remove duplicates and update target array
        remark.target = [...new Set(expandedTargets)];
      });

      return updatedData;
    }

    function splitMultiTargetRemarks(remarkData) {
      // Create a deep copy of the original data
      const splitData = JSON.parse(JSON.stringify(remarkData));
      
      // Create a new array for the split remarks
      const splitRemarks = [];
      
      splitData.remarks.forEach(remark => {
        if (Array.isArray(remark.target) && remark.target.length > 1) {
          // Split multi-target remark into separate entries
          remark.target.forEach(target => {
            splitRemarks.push({
              ...remark,
              target: target // Convert from array to single string
            });
          });
        } else {
          // Single target - convert array to string
          splitRemarks.push({
            ...remark,
            target: Array.isArray(remark.target) ? remark.target[0] : remark.target
          });
        }
      });
      
      // Update the splitData with the new remarks array
      splitData.remarks = splitRemarks;
      return splitData;
    }

    async function extractInsults(updateProgress) {
      const fileInput = document.getElementById('bookFiles');
      const selectedFiles = Array.from(fileInput.files);

      if (selectedFiles.length === 0) {
        throw new Error('Please select book files first.');
      }

      // Sort files by name to ensure proper book order
      selectedFiles.sort((a, b) => a.name.localeCompare(b.name));

      updateProgress('Reading book files...');

      const allChunkRequests = [];
      let allResults = [];
      let failedChunks = [];

      // Read all book files and create chunks
      for (let fileIndex = 0; fileIndex < selectedFiles.length; fileIndex++) {
        const file = selectedFiles[fileIndex];
        const bookNumber = fileIndex + 1;

        try {
          updateProgress(`Reading ${file.name}...`);
          const bookText = await readBookFile(file);
          const chunks = chunkText(bookText).filter((c,i) => [1, 142].includes(i));

          updateProgress(`${file.name}: Created ${chunks.length} chunks`);

          // Add all chunks for this book to the processing queue
          for (let chunkIndex = 0; chunkIndex < chunks.length; chunkIndex++) {
            allChunkRequests.push({
              book: bookNumber,
              chunkIndex: chunkIndex,
              chunkText: chunks[chunkIndex],
              previousChunkText: chunkIndex > 0 ? chunks[chunkIndex - 1] : null
            });
          }
        } catch (error) {
          console.error(`Failed to read ${file.name}:`, error);
          updateProgress(`❌ Failed to read ${file.name}: ${error.message}`);
        }
      }

      const totalChunks = allChunkRequests.length;
      let completedChunks = 0;

      updateProgress(`Processing ${totalChunks} chunks across ${selectedFiles.length} books...`);

      // Process in batches
      const batchSize = 10;
      for (let i = 0; i < allChunkRequests.length; i += batchSize) {
        const batch = allChunkRequests.slice(i, i + batchSize);
        const batchNumber = Math.floor(i / batchSize) + 1;
        const totalBatches = Math.ceil(allChunkRequests.length / batchSize);

        updateProgress(`Processing batch ${batchNumber}/${totalBatches} (${batch.length} chunks)...`);

        // Execute batch concurrently
        const batchPromises = batch.map(({ book, chunkIndex, chunkText, previousChunkText }) =>
          makeApiCall(book, chunkIndex, chunkText, previousChunkText)
        );
        const batchResults = await Promise.allSettled(batchPromises);

        // Process batch results
        for (let index = 0; index < batchResults.length; index++) {
          completedChunks++;
          const result = batchResults[index];
          const { book, chunkIndex } = batch[index];

          if (result.status === 'fulfilled' && result.value.success) {
            // Convert character names to enum values using mapping
            const conversionResult = await convertCharacterNames(result.value.data);

            if (conversionResult.success && conversionResult.mapping) {
              const updatedData = applyCharacterMapping(result.value.data, conversionResult.mapping);
              const splitData = splitMultiTargetRemarks(updatedData);
              // Preserve book and chunkIndex metadata
              splitData.book = book;
              splitData.chunkIndex = chunkIndex;
              allResults.push(splitData);
            } else {
              // If conversion fails, use original data and log warning
              console.warn(`Character conversion failed for book ${book}, chunk ${chunkIndex}: ${conversionResult.error}`);
              const splitData = splitMultiTargetRemarks(result.value.data);
              // Preserve book and chunkIndex metadata
              splitData.book = book;
              splitData.chunkIndex = chunkIndex;
              allResults.push(splitData);
            }
          } else {
            const errorMessage = result.status === 'fulfilled'
              ? result.value.error
              : result.reason?.message || 'Unknown error';
            failedChunks.push({ book, chunkIndex, error: errorMessage });
            console.error(`Failed to process book ${book}, chunk ${chunkIndex}: ${errorMessage}`);
          }
        }

        updateProgress(`Completed ${completedChunks}/${totalChunks} chunks...`);

        // Small delay between batches to be respectful to the API
        if (i + batchSize < allChunkRequests.length) {
          await new Promise(r => setTimeout(r, 1000));
        }
      }

      // Retry failed chunks one by one
      let retriedChunks = [];
      let remainingFailures = [];

      if (failedChunks.length > 0) {
        updateProgress(`Retrying ${failedChunks.length} failed chunks one by one...`);

        for (let i = 0; i < failedChunks.length; i++) {
          const { book, chunkIndex } = failedChunks[i];
          const originalChunk = allChunkRequests.find(req =>
            req.book === book && req.chunkIndex === chunkIndex
          );

          if (!originalChunk) {
            console.error(`Could not find original chunk data for book ${book}, chunk ${chunkIndex}`);
            remainingFailures.push(failedChunks[i]);
            continue;
          }

          updateProgress(`Retrying ${i + 1}/${failedChunks.length}: Book ${book}, Chunk ${chunkIndex}...`);

          try {
            const retryResult = await makeApiCall(book, chunkIndex, originalChunk.chunkText, originalChunk.previousChunkText);

            if (retryResult.success) {
              // Convert character names to enum values using mapping
              const conversionResult = await convertCharacterNames(retryResult.data);

              if (conversionResult.success && conversionResult.mapping) {
                const updatedData = applyCharacterMapping(retryResult.data, conversionResult.mapping);
                const splitData = splitMultiTargetRemarks(updatedData);
                // Preserve book and chunkIndex metadata
                splitData.book = book;
                splitData.chunkIndex = chunkIndex;
                allResults.push(splitData);
                retriedChunks.push({ book, chunkIndex });
              } else {
                // If conversion fails, use original data and log warning
                console.warn(`Character conversion failed for retry of book ${book}, chunk ${chunkIndex}: ${conversionResult.error}`);
                const splitData = splitMultiTargetRemarks(retryResult.data);
                // Preserve book and chunkIndex metadata
                splitData.book = book;
                splitData.chunkIndex = chunkIndex;
                allResults.push(splitData);
                retriedChunks.push({ book, chunkIndex });
              }
              updateProgress(`✅ Retry successful: Book ${book}, Chunk ${chunkIndex}`);
            } else {
              remainingFailures.push({ book, chunkIndex, error: retryResult.error });
              updateProgress(`❌ Retry failed: Book ${book}, Chunk ${chunkIndex} - ${retryResult.error}`);
            }
          } catch (error) {
            remainingFailures.push({ book, chunkIndex, error: error.message });
            updateProgress(`❌ Retry failed: Book ${book}, Chunk ${chunkIndex} - ${error.message}`);
          }

          // Add a small delay between retry attempts
          if (i < failedChunks.length - 1) {
            await new Promise(r => setTimeout(r, 2000));
          }
        }
      }

      // Group remaining failed chunks by book for summary
      const remainingFailedByBook = {};
      remainingFailures.forEach(({ book, chunkIndex, error }) => {
        if (!remainingFailedByBook[book]) {
          remainingFailedByBook[book] = [];
        }
        remainingFailedByBook[book].push({ chunkIndex, error });
      });

      // Group successfully retried chunks by book
      const retriedByBook = {};
      retriedChunks.forEach(({ book, chunkIndex }) => {
        if (!retriedByBook[book]) {
          retriedByBook[book] = [];
        }
        retriedByBook[book].push(chunkIndex);
      });

      return {
        results: allResults,
        failed: remainingFailedByBook,
        totalFailed: remainingFailures.length,
        totalSuccess: allResults.length,
        retriedSuccess: retriedChunks.length,
        retriedByBook: retriedByBook,
        originalFailures: failedChunks.length
      };
    }

    async function analyzeSentimentFromJSON(updateProgress) {
      const jsonFileInput = document.getElementById('jsonFile');

      if (!jsonFileInput.files.length) {
        throw new Error('Please select a JSON file first.');
      }

      const jsonFile = jsonFileInput.files[0];
      updateProgress(`Reading JSON file: ${jsonFile.name}...`);

      // Parse JSON file - now returns chunk-based structure
      const chunkBatches = await parseJSONFile(jsonFile);
      const totalRemarks = chunkBatches.reduce((sum, chunk) => sum + chunk.remarks.length, 0);
      updateProgress(`Parsed ${totalRemarks} remarks from ${chunkBatches.length} chunks`);

      if (chunkBatches.length === 0) {
        throw new Error('No chunks with remarks found in JSON file');
      }

      // Process chunks as natural batches (each chunk becomes one batch)
      const maxParallelBatches = 5; // Maximum number of chunks to process in parallel
      let allResults = [];
      let failedBatches = [];
      const totalBatches = chunkBatches.length;
      let completedBatches = 0;

      updateProgress(`Processing ${totalRemarks} remarks in ${totalBatches} chunk-based batches (max ${maxParallelBatches} parallel)...`);

      // Create chunk-based batch requests
      const allBatchRequests = chunkBatches.map((chunkBatch, index) => ({
        batch: chunkBatch.remarks, // The remarks from this chunk
        batchIndex: index,
        chunkInfo: {
          book: chunkBatch.book,
          chunkKey: chunkBatch.chunkKey,
          chunk_text: chunkBatch.chunk_text
        }
      }));

      // Process batches in parallel groups
      for (let i = 0; i < allBatchRequests.length; i += maxParallelBatches) {
        const parallelBatches = allBatchRequests.slice(i, i + maxParallelBatches);
        const groupNumber = Math.floor(i / maxParallelBatches) + 1;
        const totalGroups = Math.ceil(allBatchRequests.length / maxParallelBatches);

        updateProgress(`Processing chunk group ${groupNumber}/${totalGroups} (${parallelBatches.length} chunks in parallel)...`);

        // Execute chunk-based batches in parallel
        const batchPromises = parallelBatches.map(({ batch, batchIndex, chunkInfo }) =>
          makeSentimentAnalysisCall(batch, batchIndex, chunkInfo).then(result => ({
            ...result,
            chunkInfo: chunkInfo
          }))
        );
        const batchResults = await Promise.allSettled(batchPromises);

        // Process results
        for (let j = 0; j < batchResults.length; j++) {
          completedBatches++;
          const result = batchResults[j];
          const { batch, batchIndex, chunkInfo } = parallelBatches[j];

          if (result.status === 'fulfilled' && result.value.success) {
            allResults.push({
              ...result.value.data,
              chunkInfo: result.value.chunkInfo
            });
            updateProgress(`✅ Completed chunk ${batchIndex + 1}/${totalBatches} (${chunkInfo.chunkKey}) in group ${groupNumber}`);
          } else {
            const errorMessage = result.status === 'fulfilled'
              ? result.value.error
              : result.reason?.message || 'Unknown error';
            failedBatches.push({ batchIndex, batch, chunkInfo, error: errorMessage });
            updateProgress(`❌ Failed chunk ${batchIndex + 1}/${totalBatches} (${chunkInfo.chunkKey}): ${errorMessage}`);
          }
        }

        updateProgress(`Completed group ${groupNumber}/${totalGroups} - ${completedBatches}/${totalBatches} batches processed`);

        // Small delay between parallel groups to be respectful to the API
        if (i + maxParallelBatches < allBatchRequests.length) {
          await new Promise(r => setTimeout(r, 1000));
        }
      }

      // Retry failed batches one by one
      let retriedBatches = [];
      let remainingFailures = [];

      if (failedBatches.length > 0) {
        updateProgress(`Retrying ${failedBatches.length} failed chunks...`);

        for (let i = 0; i < failedBatches.length; i++) {
          const { batchIndex, batch, chunkInfo, error } = failedBatches[i];

          updateProgress(`Retrying ${i + 1}/${failedBatches.length}: Chunk ${batchIndex} (${chunkInfo.chunkKey})...`);

          try {
            const retryResult = await makeSentimentAnalysisCall(batch, batchIndex, chunkInfo);

            if (retryResult.success) {
              allResults.push({
                ...retryResult.data,
                chunkInfo: chunkInfo
              });
              retriedBatches.push({ batchIndex, chunkInfo });
              updateProgress(`✅ Retry successful: Chunk ${batchIndex} (${chunkInfo.chunkKey})`);
            } else {
              remainingFailures.push({ batchIndex, batch, chunkInfo, error: retryResult.error });
              updateProgress(`❌ Retry failed: Chunk ${batchIndex} (${chunkInfo.chunkKey}) - ${retryResult.error}`);
            }
          } catch (retryError) {
            remainingFailures.push({ batchIndex, batch, chunkInfo, error: retryError.message });
            updateProgress(`❌ Retry failed: Chunk ${batchIndex} (${chunkInfo.chunkKey}) - ${retryError.message}`);
          }

          // Add a delay between retry attempts
          if (i < failedBatches.length - 1) {
            await new Promise(r => setTimeout(r, 2000));
          }
        }
      }

      // Flatten the chunk-based results back to a flat array for downstream processing
      const flattenedOriginalData = chunkBatches.reduce((acc, chunk) => {
        acc.push(...chunk.remarks);
        return acc;
      }, []);

      return {
        results: allResults,
        originalData: flattenedOriginalData, // Flattened for downstream processing
        chunkBatches: chunkBatches, // Include chunk batch information
        totalItems: totalRemarks,
        totalBatches: totalBatches,
        successfulBatches: allResults.length,
        originalFailures: failedBatches.length,
        retriedSuccess: retriedBatches.length,
        remainingFailures: remainingFailures.length,
        failedBatches: remainingFailures
      };
    }

    // Global variables to store the last results for downloading
    let lastResults = null;
    let lastResultsType = null; // 'extraction' or 'sentiment'
    let originalFormat = null; // 'json' - tracks the original input format

    // Download utility functions
    function downloadFile(data, filename, mimeType) {
      const blob = new Blob([data], { type: mimeType });
      const url = URL.createObjectURL(blob);
      const link = document.createElement('a');
      link.href = url;
      link.download = filename;
      document.body.appendChild(link);
      link.click();
      document.body.removeChild(link);
      URL.revokeObjectURL(url);
    }

    function generateTimestamp() {
      const now = new Date();
      return now.getFullYear() +
             String(now.getMonth() + 1).padStart(2, '0') +
             String(now.getDate()).padStart(2, '0') + '_' +
             String(now.getHours()).padStart(2, '0') +
             String(now.getMinutes()).padStart(2, '0') +
             String(now.getSeconds()).padStart(2, '0');
    }


    function showDownloadSection() {
      document.getElementById('downloadSection').style.display = 'block';
    }

    function hideDownloadSection() {
      document.getElementById('downloadSection').style.display = 'none';
    }

    // Download button event listener
    document.getElementById('downloadBtn').addEventListener('click', () => {
      if (!lastResults) {
        alert('No results to download. Please run extraction or sentiment analysis first.');
        return;
      }

      const timestamp = generateTimestamp();

      // All results are downloaded as JSON
      const filename = `hp_${lastResultsType}_results_${timestamp}.json`;
      const jsonString = JSON.stringify(lastResults, null, 2);
      downloadFile(jsonString, filename, 'application/json');
    });

    document.getElementById("startBtn").addEventListener("click", async () => {
      const progressEl = document.getElementById("progress");
      const outputEl = document.getElementById("output");
      const apiKey = getApiKey();

      // Validate inputs
      if (!apiKey) {
        progressEl.textContent = "❌ Please enter your OpenAI API key before starting extraction.";
        progressEl.style.color = "red";
        return;
      }

      if (!apiKey.startsWith('sk-')) {
        progressEl.textContent = "❌ Invalid API key format. OpenAI API keys start with 'sk-'.";
        progressEl.style.color = "red";
        return;
      }

      const fileInput = document.getElementById('bookFiles');
      const jsonFileInput = document.getElementById('jsonFile');

      // Check that either book files OR JSON file is provided
      if (fileInput.files.length === 0 && jsonFileInput.files.length === 0) {
        progressEl.textContent = "❌ Please select either book files for extraction OR a JSON file for analysis.";
        progressEl.style.color = "red";
        return;
      }

      // If only JSON is provided, redirect to sentiment analysis
      if (fileInput.files.length === 0 && jsonFileInput.files.length > 0) {
        progressEl.textContent = "ℹ️ JSON file detected. Use 'Analyze Sentiment from Data' button for sentiment analysis.";
        progressEl.style.color = "orange";
        return;
      }

      // Reset progress color and hide download section
      progressEl.style.color = "";
      progressEl.textContent = "Starting extraction...";
      hideDownloadSection();

      try {
        const extractionResults = await extractInsults(msg => {
          progressEl.textContent = msg;
        });

        // Create summary message with retry information - will be updated after flattening
        let summary = `Done! Successfully processed ${extractionResults.totalSuccess} chunks.`;
        if (extractionResults.originalFailures > 0) {
          summary += ` Originally ${extractionResults.originalFailures} chunks failed.`;
          if (extractionResults.retriedSuccess > 0) {
            summary += ` Successfully retried ${extractionResults.retriedSuccess} chunks.`;
          }
          if (extractionResults.totalFailed > 0) {
            summary += ` ${extractionResults.totalFailed} chunks remain failed after retry.`;
          } else {
            summary += ` All failed chunks were successfully retried!`;
          }
        }

        // Create flat list of chunks
        const chunksList = [];
        let totalRemarks = 0;

        extractionResults.results.forEach(chunkResult => {
          // Create flat chunk structure
          const chunk = {
            book: chunkResult.book || 'unknown',
            chunk_index: chunkResult.chunkIndex !== undefined ? chunkResult.chunkIndex : 'undefined',
            chunk_text: chunkResult.chunk_text || '',
            remarks: chunkResult.remarks || []
          };

          chunksList.push(chunk);

          // Count total remarks
          if (chunkResult.remarks && Array.isArray(chunkResult.remarks)) {
            totalRemarks += chunkResult.remarks.length;
          }
        });

        // Prepare output with results and retry summary
        let output = {
          chunks: chunksList,
          summary: {
            total_chunks: extractionResults.totalSuccess + extractionResults.totalFailed,
            successful: extractionResults.totalSuccess,
            total_remarks: totalRemarks,
            original_failures: extractionResults.originalFailures,
            retried_successes: extractionResults.retriedSuccess,
            remaining_failures: extractionResults.totalFailed
          }
        };

        // Add retry success summary if there were successful retries
        if (extractionResults.retriedSuccess > 0) {
          output.successfully_retried_chunks_by_book = {};
          Object.entries(extractionResults.retriedByBook).forEach(([book, chunkIndexes]) => {
            output.successfully_retried_chunks_by_book[`Book ${book}`] = {
              retried_count: chunkIndexes.length,
              chunk_indexes: chunkIndexes
            };
          });
        }

        // Add remaining failed chunks summary if there are any remaining failures
        if (extractionResults.totalFailed > 0) {
          output.remaining_failed_chunks_by_book = {};
          Object.entries(extractionResults.failed).forEach(([book, chunks]) => {
            output.remaining_failed_chunks_by_book[`Book ${book}`] = {
              failed_count: chunks.length,
              chunks: chunks.map(ch => `Chunk ${ch.chunkIndex}: ${ch.error}`)
            };
          });
        }

        // Update progress message with total remarks extracted
        summary += ` Extracted ${totalRemarks} individual remarks with chunk text included.`;
        progressEl.textContent = summary;

        outputEl.textContent = JSON.stringify(output, null, 2);

        // Store results for downloading and show download section
        lastResults = output;
        lastResultsType = 'extraction';
        originalFormat = 'json'; // Extraction from books creates JSON results
        document.getElementById('downloadInfo').textContent = 'Results will be downloaded as JSON (original format from book extraction)';
        showDownloadSection();
      } catch (error) {
        progressEl.textContent = `❌ Error: ${error.message}`;
        progressEl.style.color = "red";
        console.error('Extraction error:', error);
      }
    });

    document.getElementById("analyzeSentimentBtn").addEventListener("click", async () => {
      const progressEl = document.getElementById("progress");
      const outputEl = document.getElementById("output");
      const apiKey = getApiKey();

      // Validate inputs
      if (!apiKey) {
        progressEl.textContent = "❌ Please enter your OpenAI API key before starting sentiment analysis.";
        progressEl.style.color = "red";
        return;
      }

      if (!apiKey.startsWith('sk-')) {
        progressEl.textContent = "❌ Invalid API key format. OpenAI API keys start with 'sk-'.";
        progressEl.style.color = "red";
        return;
      }

      const jsonFileInput = document.getElementById('jsonFile');
      const fileInput = document.getElementById('bookFiles');

      // Check that either JSON file OR book files is provided
      if (jsonFileInput.files.length === 0 && fileInput.files.length === 0) {
        progressEl.textContent = "❌ Please select either a JSON file for analysis OR book files for extraction.";
        progressEl.style.color = "red";
        return;
      }

      // If only book files are provided, redirect to extraction
      if (jsonFileInput.files.length === 0 && fileInput.files.length > 0) {
        progressEl.textContent = "ℹ️ Book files detected. Use 'Start Extraction' button first to extract remarks, then analyze sentiment.";
        progressEl.style.color = "orange";
        return;
      }

      // Reset progress color and hide download section
      progressEl.style.color = "";
      progressEl.textContent = "Starting sentiment analysis...";
      hideDownloadSection();

      try {
        const analysisResults = await analyzeSentimentFromJSON(msg => {
          progressEl.textContent = msg;
        });

        // Create summary message (will be updated after processing)
        let summary = `Processing complete! Preparing results...`;
        progressEl.textContent = summary;

        // Create enhanced JSON output with original data plus new sentiment analysis
        const enhancedResults = [...analysisResults.originalData]; // Start with all original items

        // Create a map to track which items have been processed
        const processedItems = new Map();

        // Process all successfully analyzed items - using chunk-aware indexing
        let globalIndexOffset = 0;
        const chunkSizes = analysisResults.chunkBatches.map(chunk => chunk.remarks.length);

        analysisResults.results.forEach(batchResult => {
          // Calculate the starting index for this chunk/batch
          let startIndex = 0;
          for (let i = 0; i < batchResult.batch_index; i++) {
            startIndex += chunkSizes[i] || 0;
          }

          batchResult.analyzed_items.forEach(item => {
            // Calculate the correct global index for this item
            const globalIndex = startIndex + item.item_index;
            processedItems.set(globalIndex, {
              sentiment: item.sentiment,
              reasoning: item.reasoning
            });
          });
        });

        // Apply the analysis results to the corresponding original items
        enhancedResults.forEach((originalItem, index) => {
          if (processedItems.has(index)) {
            const analysisResult = processedItems.get(index);
            originalItem.sentiment = analysisResult.sentiment;
            originalItem.reasoning = analysisResult.reasoning;
          } else {
            // Item wasn't processed (likely due to batch failure)
            originalItem.sentiment = "";
            originalItem.reasoning = "Not processed due to batch failure";
          }
        });

        // Count processed vs unprocessed items
        const processedCount = enhancedResults.filter(item => item.sentiment && item.sentiment !== "").length;
        const unprocessedCount = enhancedResults.filter(item => !item.sentiment || item.sentiment === "").length;

        // Prepare output
        let output = {
          enhanced_remarks: enhancedResults,
          summary: {
            total_items: analysisResults.totalItems,
            processed_items: processedCount,
            unprocessed_items: unprocessedCount,
            total_batches: analysisResults.totalBatches,
            successful_batches: analysisResults.successfulBatches,
            original_failures: analysisResults.originalFailures,
            retried_successes: analysisResults.retriedSuccess,
            remaining_failures: analysisResults.remainingFailures
          },
          data_fields: ["book", "speaker", "target", "sentiment", "description", "reasoning"]
        };

        // Add failed batches information if there are remaining failures
        if (analysisResults.remainingFailures > 0) {
          output.failed_batches = analysisResults.failedBatches.map(fb => ({
            batch_index: fb.batchIndex,
            error: fb.error,
            items_count: fb.batch.length
          }));
        }

        outputEl.textContent = JSON.stringify(output, null, 2);

        // Store results for downloading and show download section
        lastResults = output;
        lastResultsType = 'sentiment';
        originalFormat = 'json'; // Sentiment analysis from JSON creates enhanced JSON results
        document.getElementById('downloadInfo').textContent = 'Results will be downloaded as JSON (enhanced with sentiment analysis)';
        showDownloadSection();

        // Update final summary message
        let finalSummary = `Done! Successfully analyzed ${processedCount}/${analysisResults.totalItems} items.`;
        if (unprocessedCount > 0) {
          finalSummary += ` ${unprocessedCount} items could not be processed due to batch failures.`;
        }
        if (analysisResults.originalFailures > 0) {
          finalSummary += ` (Originally ${analysisResults.originalFailures} batches failed`;
          if (analysisResults.retriedSuccess > 0) {
            finalSummary += `, ${analysisResults.retriedSuccess} successfully retried`;
          }
          finalSummary += `)`;
        }
        progressEl.textContent = finalSummary;
      } catch (error) {
        progressEl.textContent = `❌ Error: ${error.message}`;
        progressEl.style.color = "red";
        console.error('Sentiment analysis error:', error);
      }
    });

    // Initialize character names on page load
    window.addEventListener('DOMContentLoaded', function() {
      loadCharacterNames();
    });
  </script>
</body>
</html>
